networks:
  openlpr-network:
    driver: bridge

volumes:
  lpr_data:
    driver: local
  lpr_media:
    driver: local
  lpr_static:
    driver: local
  model_files:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

services:
  # Core Services
  traefik:
    image: traefik:v3.0
    container_name: openlpr-traefik
    command:
      - "--api.insecure=true"
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"
      - "--entrypoints.web.address=:80"
      - "--providers.file.directory=/etc/traefik/dynamic"
      - "--providers.file.watch=true"
    ports:
      - "80:80"
      - "8080:8080"  # Traefik dashboard
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./traefik/traefik.yml:/etc/traefik/traefik.yml:ro
      - ./traefik/dynamic:/etc/traefik/dynamic:ro
    networks:
      - openlpr-network
    restart: unless-stopped
    profiles:
      - core
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.traefik.rule=Host(`traefik.localhost`)"
      - "traefik.http.routers.traefik.service=api@internal"
      - "traefik.http.services.traefik.loadbalancer.server.port=8080"

  lpr-app:
    env_file:
      - .env.llamacpp
    image: ghcr.io/faisalthaheem/open-lpr:latest
    container_name: open-lpr-app
    volumes:
      # SQLite database persistence
      - ./container-data:/app/data
      # Media files persistence (uploaded and processed images)
      - ./container-media:/app/media
      # Static files (optional - for development)
      - ./staticfiles:/app/staticfiles
    # Run as root initially for setup, then entrypoint switches to django user
    user: "0:0"
    environment:
      # Django Settings
      - SECRET_KEY=${SECRET_KEY:-django-insecure-change-me-in-production}
      - DEBUG=${DEBUG:-False}
      - ALLOWED_HOSTS=${ALLOWED_HOSTS:-localhost,127.0.0.1,0.0.0.0}
      
      # Qwen3-VL API Configuration - Updated to use local LlamaCpp service
      - QWEN_API_KEY=${QWEN_API_KEY:-sk-llamacpp-local}
      - QWEN_BASE_URL=${QWEN_BASE_URL:-http://llamacpp:8000/v1}
      - QWEN_MODEL=${QWEN_MODEL:-gpt-4-vision-preview}  # Model alias for compatibility
      
      # File Upload Settings
      - UPLOAD_FILE_MAX_SIZE=${UPLOAD_FILE_MAX_SIZE:-10485760}
      - MAX_BATCH_SIZE=${MAX_BATCH_SIZE:-10}
      
      # Database Configuration
      - DATABASE_PATH=/app/data/db.sqlite3
      
      # Optional: Superuser creation
      - DJANGO_SUPERUSER_USERNAME=${DJANGO_SUPERUSER_USERNAME:-}
      - DJANGO_SUPERUSER_EMAIL=${DJANGO_SUPERUSER_EMAIL:-}
      - DJANGO_SUPERUSER_PASSWORD=${DJANGO_SUPERUSER_PASSWORD:-}
    networks:
      - openlpr-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "manage.py", "check"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    entrypoint: ["./docker-entrypoint.sh"]
    command: ["gunicorn", "--bind", "0.0.0.0:8000", "--workers", "3", "--timeout", "120", "lpr_project.wsgi:application"]
    profiles:
      - core
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.lpr-app.rule=Host(`lpr.localhost`)"
      - "traefik.http.services.lpr-app.loadbalancer.server.port=8000"

  prometheus:
    image: prom/prometheus:latest
    container_name: openlpr-prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    networks:
      - openlpr-network
    restart: unless-stopped
    profiles:
      - core
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.prometheus.rule=Host(`prometheus.localhost`)"
      - "traefik.http.services.prometheus.loadbalancer.server.port=9090"

  grafana:
    image: grafana/grafana:latest
    container_name: openlpr-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
    networks:
      - openlpr-network
    restart: unless-stopped
    profiles:
      - core
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.grafana.rule=Host(`grafana.localhost`)"
      - "traefik.http.services.grafana.loadbalancer.server.port=3000"

  # Inference Services
  llamacpp-cpu:
    env_file:
      - .env.llamacpp
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: llamacpp-qwen3vl-cpu
    ports:
      - "8001:8000"  # Use 8001 to avoid conflict with OpenLPR on 8000
    volumes:
      # Model storage
      - ./model_files:/models
      - ./model_files_cache:/root/.cache/
    environment:
      # Model configuration
      - LLAMA_ARG_MODEL=/models/${MODEL_FILE:-Qwen3-VL-4B-Instruct-Q5_K_M.gguf}
      - LLAMA_ARG_HF_REPO=${MODEL_REPO:-unsloth/Qwen3-VL-4B-Instruct-GGUF}
      - LLAMA_ARG_HF_FILE=${MODEL_FILE:-Qwen3-VL-4B-Instruct-Q5_K_M.gguf}
      - LLAMA_ARG_MMPROJ_URL=${MMPROJ_URL:-https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-GGUF/resolve/main/mmproj-BF16.gguf}
      - HF_TOKEN=${HF_TOKEN}
      - LLAMA_ARG_JINJA=1
      - LLAMA_ARG_CHAT_TEMPLATE=chatml
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_ARG_PORT=8000
    networks:
      - openlpr-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    profiles:
      - cpu

  llamacpp-amd-vulkan:
    env_file:
      - .env.llamacpp
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: llamacpp-qwen3vl-amd-vulkan
    ports:
      - "8001:8000"  # Use 8001 to avoid conflict with OpenLPR on 8000
    volumes:
      # Model storage
      - ./model_files:/models
      - ./model_files_cache:/root/.cache/
    environment:
      # Model configuration
      - LLAMA_ARG_MODEL=/models/${MODEL_FILE:-Qwen3-VL-4B-Instruct-Q5_K_M.gguf}
      - LLAMA_ARG_HF_REPO=${MODEL_REPO:-unsloth/Qwen3-VL-4B-Instruct-GGUF}
      - LLAMA_ARG_HF_FILE=${MODEL_FILE:-Qwen3-VL-4B-Instruct-Q5_K_M.gguf}
      - LLAMA_ARG_MMPROJ_URL=${MMPROJ_URL:-https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-GGUF/resolve/main/mmproj-BF16.gguf}
      - HF_TOKEN=${HF_TOKEN}
      - LLAMA_ARG_JINJA=1
      - LLAMA_ARG_CHAT_TEMPLATE=chatml
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_ARG_PORT=8000
      - LLAMA_ARG_DEVICE=vulkan0
    networks:
      - openlpr-network
    devices:
      - /dev/kfd
      - /dev/dri
    security_opt:
      - seccomp=unconfined
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    profiles:
      - amd-vulkan

  llamacpp-nvidia-cuda:
    env_file:
      - .env.llamacpp
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: llamacpp-qwen3vl-nvidia-cuda
    ports:
      - "8001:8000"  # Use 8001 to avoid conflict with OpenLPR on 8000
    volumes:
      # Model storage
      - ./model_files:/models
      - ./model_files_cache:/root/.cache/
    environment:
      # Model configuration
      - LLAMA_ARG_MODEL=/models/${MODEL_FILE:-Qwen3-VL-4B-Instruct-Q5_K_M.gguf}
      - LLAMA_ARG_HF_REPO=${MODEL_REPO:-unsloth/Qwen3-VL-4B-Instruct-GGUF}
      - LLAMA_ARG_HF_FILE=${MODEL_FILE:-Qwen3-VL-4B-Instruct-Q5_K_M.gguf}
      - LLAMA_ARG_MMPROJ_URL=${MMPROJ_URL:-https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-GGUF/resolve/main/mmproj-BF16.gguf}
      - HF_TOKEN=${HF_TOKEN}
      - LLAMA_ARG_JINJA=1
      - LLAMA_ARG_CHAT_TEMPLATE=chatml
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_ARG_PORT=8000
      - LLAMA_ARG_N_GPUS=1
      - LLAMA_ARG_GPU_LAYERS=99
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - openlpr-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    profiles:
      - nvidia-cuda
