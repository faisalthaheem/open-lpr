version: '3.8'

networks:
  openlpr-network:
    driver: bridge

volumes:
  lpr_data:
    driver: local
  lpr_media:
    driver: local
  lpr_static:
    driver: local
  model_files:
    driver: local

services:
  # LlamaCpp Service - Qwen3-VL-4B Inference Server
  llamacpp:
    env_file:
      - .env.llamacpp
    image: ghcr.io/ggml-org/llama.cpp:server-vulkan
    container_name: llamacpp-qwen3vl
    ports:
      - "8001:8000"  # Use 8001 to avoid conflict with OpenLPR on 8000
    volumes:
      # Model storage
      - ./model_files:/models
      - ./model_files_cache:/root/.cache/
    environment:
      # Model configuration
      - LLAMA_ARG_MODEL=/models/${MODEL_FILE:-Qwen3-VL-4B-Instruct-Q5_K_M.gguf}
      - LLAMA_ARG_HF_REPO=${MODEL_REPO:-unsloth/Qwen3-VL-4B-Instruct-GGUF}
      - LLAMA_ARG_HF_FILE=${MODEL_FILE:-Qwen3-VL-4B-Instruct-Q5_K_M.gguf}
      - LLAMA_ARG_MMPROJ_URL=${MMPROJ_URL:-https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-GGUF/resolve/main/mmproj-BF16.gguf}
      - HF_TOKEN=${HF_TOKEN}
      - LLAMA_ARG_JINJA=1
      - LLAMA_ARG_CHAT_TEMPLATE=chatml
      - LLAMA_ARG_HOST=0.0.0.0
      - LLAMA_ARG_PORT=8000
      - LLAMA_ARG_DEVICE=vulkan0
    networks:
      - openlpr-network
    devices:
      - /dev/kfd
      - /dev/dri
    security_opt:
      - seccomp=unconfined
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    

  # OpenLPR Service - Existing Django Application
  lpr-app:
    env_file:
      - .env.llamacpp
    image: ghcr.io/faisalthaheem/open-lpr:latest
    container_name: open-lpr-app
    ports:
      - "8000:8000"
    volumes:
      # SQLite database persistence
      - ./container-data:/app/data
      # Media files persistence (uploaded and processed images)
      - ./container-media:/app/media
      # Static files (optional - for development)
      - ./staticfiles:/app/staticfiles
    # Run as root initially for setup, then entrypoint switches to django user
    user: "0:0"
    environment:
      # Django Settings
      - SECRET_KEY=${SECRET_KEY:-django-insecure-change-me-in-production}
      - DEBUG=${DEBUG:-False}
      - ALLOWED_HOSTS=${ALLOWED_HOSTS:-localhost,127.0.0.1,0.0.0.0}
      
      # Qwen3-VL API Configuration - Updated to use local LlamaCpp service
      - QWEN_API_KEY=sk-llamacpp-local
      - QWEN_BASE_URL=http://llamacpp:8000/v1
      - QWEN_MODEL=gpt-4-vision-preview  # Model alias for compatibility
      
      # File Upload Settings
      - UPLOAD_FILE_MAX_SIZE=${UPLOAD_FILE_MAX_SIZE:-10485760}
      - MAX_BATCH_SIZE=${MAX_BATCH_SIZE:-10}
      
      # Database Configuration
      - DATABASE_PATH=/app/data/db.sqlite3
      
      # Optional: Superuser creation
      - DJANGO_SUPERUSER_USERNAME=${DJANGO_SUPERUSER_USERNAME:-}
      - DJANGO_SUPERUSER_EMAIL=${DJANGO_SUPERUSER_EMAIL:-}
      - DJANGO_SUPERUSER_PASSWORD=${DJANGO_SUPERUSER_PASSWORD:-}
    networks:
      - openlpr-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "manage.py", "check"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    depends_on:
      llamacpp:
        condition: service_healthy
    entrypoint: ["./docker-entrypoint.sh"]
    command: ["gunicorn", "--bind", "0.0.0.0:8000", "--workers", "3", "--timeout", "120", "lpr_project.wsgi:application"]

  # Optional: Nginx Reverse Proxy for Production
  nginx:
    env_file:
      - .env.llamacpp
    image: nginx:alpine
    container_name: openlpr-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - ./staticfiles:/var/www/static:ro
    networks:
      - openlpr-network
    restart: unless-stopped
    depends_on:
      - lpr-app
      - llamacpp
    profiles:
      - production  # Only start with --profile production